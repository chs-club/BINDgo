{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 Basic classifier\n",
    "\n",
    "Developing predictive models.\n",
    "\n",
    "Essential steps: data preprocessing, feature engineering, model selection, training, and evaluation.\n",
    "\n",
    "The goal is to create a machine learning model that can predict the binding affinity of small molecules to a single protein target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The system path, managed by the `sys.path` variable in Python, is a list of directory paths that the Python interpreter searches to locate modules and packages when an `import` statement is executed.\n",
    "\n",
    "Current `sys.path` is able to show which directories are being searched by default "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/opt/anaconda3/envs/leash_bio_kaggle-dev/lib/python311.zip', '/opt/anaconda3/envs/leash_bio_kaggle-dev/lib/python3.11', '/opt/anaconda3/envs/leash_bio_kaggle-dev/lib/python3.11/lib-dynload', '', '/opt/anaconda3/envs/leash_bio_kaggle-dev/lib/python3.11/site-packages', '/Users/ria/leash-bio-kaggle']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This list contains the directories that Python searches for modules.\n",
    "Here’s a brief explanation of some key elements in this list:\n",
    "\n",
    "-   `'/opt/anaconda3/envs/leash_bio_kaggle-dev/lib/python311.zip'`: This is the path to the Python standard library zip archive, which contains standard modules.\n",
    "-   `'/opt/anaconda3/envs/leash_bio_kaggle-dev/lib/python3.11'`: This is the directory containing the core Python library.\n",
    "-   `'/opt/anaconda3/envs/leash_bio_kaggle-dev/lib/python3.11/lib-dynload'`: This directory contains dynamic modules (e.g., compiled extension modules).\n",
    "-   `''`: The empty string represents the current directory, allowing Python to search for modules in the directory from which the script is run.\n",
    "-   `'/opt/anaconda3/envs/leash_bio_kaggle-dev/lib/python3.11/site-packages'`: This is the directory for third-party packages installed in the current virtual environment.\n",
    "\n",
    "`'/Users/ria/leash-bio-kaggle'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative imports\n",
    "\n",
    "In Python, the `os.path.abspath` function is used to obtain the absolute path of a given path.\n",
    "An absolute path is a complete path from the root of the file system to the desired directory or file, leaving no ambiguity about its location.\n",
    "\n",
    "#### Why use `abspath`?\n",
    "\n",
    "`abspath` Absolute paths ensure that the path to the module is always interpreted correctly, regardless of the current working directory.\n",
    "\n",
    "Crucial in dynamic environments like Jupyter notebooks where the working directory can change depending on how the notebook is executed.\n",
    "\n",
    "#### Why Use Relative Path Import?\n",
    "\n",
    "Using a relative path in conjunction with `os.path.abspath` helps maintain the portability of the project.\n",
    "The relative path (`'../../../'`) specifies the location of the `leash_bio_kaggle` module relative to the current file's location.\n",
    "This approach makes it easy to move the entire project to a different directory or system without breaking the import paths.\n",
    "\n",
    "This practice helps in organizing the project structure logically.\n",
    "By using relative paths, we can easily locate and manage modules within the project hierarchy.\n",
    "It also keeps the code cleaner and more understandable, as the structure is evident from the path specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ria/leash-bio-kaggle/leash_bio_kaggle\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../../../', 'leash_bio_kaggle'))\n",
    "print(module_path)\n",
    "\n",
    "if not os.path.exists(module_path):\n",
    "    raise RuntimeError(\"Cannot find the Python module `leash_bio_kaggle`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After determining the correct path to our custom module using `os.path.abspath`, the next step is to ensure that this path is included in the sys.path list.\n",
    "Let’s see how we can add our module path to the system path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if our custom module path is already present in the `sys.path`.\n",
    "\n",
    "Prevents adding the same path multiple times\n",
    "\n",
    "If the custom module path is not found in `sys.path`, we append it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and processing\n",
    "\n",
    "PyArrow loads our dataset and is a powerful library for working with large, columnar data structures, and it provides efficient tools for reading and writing data in the Parquet format, among others.\n",
    "\n",
    "First, we import the necessary module from PyArrow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.dataset as ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the path to our training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TRAIN_DATA = \"../../../data/train.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we load the dataset using the PyArrow dataset function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = ds.dataset(source=PATH_TRAIN_DATA, format=\"parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Protein selection\n",
    "\n",
    "In this section, we will focus on a single protein target from our dataset. The dataset contains binding affinity data for three different proteins. To simplify our approach and make it more manageable for this beginner tutorial, we will select only one protein target. This will help us avoid overcomplicating our analysis while still demonstrating the essential concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_selection = \"sEH\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scanner allow us to query and load only the necessary parts of the dataset into memory.\n",
    "\n",
    "pyarrow.compute module allows us to apply complex filters directly on the dataset without loading it entirely into memory.  It provides a set of functions for performing computations and filtering on Arrow arrays and tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.compute as pc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 2 separate scanners to filter the dataset based on the protein selection an binding status: molecules that bind, and molecules that do not.\n",
    "\n",
    "The filters use boolean conditions to specify the criteria for selecting rows, with the `&` operator combining these conditions. \n",
    "\n",
    "Allows us to efficiently handle the large dataset by focusing our analysis on a specific protein and its binding characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scanner_protein_bind = DATA.scanner(\n",
    "    filter=(pc.field(\"protein_name\") == protein_selection) & (pc.field(\"binds\") == 1)\n",
    ")\n",
    "scanner_protein_no_bind = DATA.scanner(\n",
    "    filter=(pc.field(\"protein_name\") == protein_selection) & (pc.field(\"binds\") == 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   `scanner_protein_bind`: This scanner filters the dataset to include only rows where the selected protein (sEH) binds (i.e., `binds` is `1`). This subset of data will be used to analyze molecules that successfully bind to the protein.\n",
    "-   `scanner_protein_no_bind`: This scanner filters the dataset to include only rows where the selected protein (sEH) does not bind (i.e., `binds` is `0`). This subset will be used to analyze molecules that do not bind to the protein."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsampling dataset\n",
    "\n",
    "* Subsampling involves selecting a smaller, representative portion of the data for analysis or training. \n",
    "* Creates a more balanced dataset that allows the machine learning model to learn the characteristics of both classes more effectively.\n",
    "* A representative subsample of the data can allow good generalization\n",
    "* A well-chosen subsample can capture the essential patterns and variations in the data, enabling the model to perform well on unseen data. \n",
    "\n",
    "\n",
    "Working with a large dataset containing binding affinity data for molecules against a specific protein. \n",
    "* The dataset is likely imbalanced, with more instances of molecules that do not bind to the protein compared to those that do. \n",
    "    * This can lead to a biased model\n",
    "* Create a balanced subset of the data for our initial analysis and model training.\n",
    "\n",
    "2 scanners: one for binding molecules and one for non-binding molecules.\n",
    "* By subsampling these datasets, we can ensure a more balanced representation of both classes, making our machine learning task more tractable and improving the model's performance.\n",
    "\n",
    "#### Counting Rows\n",
    "\n",
    "First, we count the number of rows (samples) for binding and non-binding molecules.\n",
    "`count_rows()` is a method counts the number of rows in the scanner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows_bind = scanner_protein_bind.count_rows()\n",
    "n_rows_no_bind = scanner_protein_no_bind.count_rows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with large and potentially imbalanced datasets, it's important to choose sample sizes that create a balanced and manageable subset for analysis and model training. \n",
    "* In this example, we make specific choices for the number of binding and non-binding samples, but these numbers can be adjusted based on the characteristics of your dataset and the goals of your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bind = 10_000\n",
    "ratio_no_bind = 1.0\n",
    "n_no_bind = int(ratio_no_bind * n_bind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  Setting the Number of Binding Samples (`n_bind`).\n",
    "\n",
    "    We set the desired number of binding molecule samples to 10,000. This number is chosen to provide a substantial amount of data for training and validating our machine learning model while keeping the dataset size manageable.\n",
    "    * The choice of 10,000 is somewhat arbitrary and should be based on the size of your dataset and the computational resources available. \n",
    "2.  Setting the Ratio of Non-Binding to Binding Samples (`ratio_no_bind`).\n",
    "\n",
    "    We set the ratio of non-binding to binding samples to 1.0, meaning we want an equal number of non-binding samples as binding samples. T\n",
    "    * This creates a balanced dataset, which is important for many machine learning algorithms that perform better when classes are balanced.\n",
    "    \n",
    "    Ratio chosen based on the level of imbalance in your dataset. \n",
    "    * For highly imbalanced datasets: ratio of less than 1 to downsample the majority class or use techniques like oversampling the minority class. \n",
    "    * A ratio of 1.0 is a good starting point for creating balanced datasets\n",
    "3.  Calculating the Number of Non-Binding Samples (`n_no_bind`).\n",
    "    Calculated based on the specified ratio. \n",
    "    * In this case, with a ratio of 1.0 and n_bind of 10,000, `n_no_bind` will also be 10,000, resulting in a balanced subset.\n",
    "    \n",
    "    Ensure that the calculated number of samples does not exceed the available data. \n",
    "    * If your dataset has fewer non-binding samples than calculated, you may need to adjust your ratio or consider techniques like upsampling the minority class. \n",
    "    * Always verify that the chosen sample sizes are feasible given the dataset's composition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_bind > n_rows_bind:\n",
    "    raise ValueError(\"`n_bind` is greater than `n_samples_bind`\")\n",
    "if n_no_bind > n_rows_no_bind:\n",
    "    raise ValueError(\"`n_no_bind` is greater than `n_samples_no_bind`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we proceed with subsampling the dataset, we need to import the necessary libraries. In this example, we use NumPy and PyArrow to perform random sampling and handle data efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyarrow as pa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we perform random sampling on the dataset to select a specified number of binding molecule samples. This step ensures that we have a manageable subset of binding data for further analysis and model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bind_indices = np.random.choice(n_rows_bind, size=n_bind, replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code block, we use the `np.random.choice` function to generate an array of random indices. Specifically, `np.random.choice(n_rows_bind, size=n_bind, replace=False)` generates `n_bind` random indices from the total number of binding samples (`n_rows_bind`). The `replace=False` parameter ensures that each index is unique, meaning the same index is not selected more than once. This method of random sampling helps in creating a diverse and representative subset of binding data.\n",
    "\n",
    "Once we have the random indices, we use the take method of the scanner object to extract the corresponding rows from the dataset. The `scanner_protein_bind.take(indices=...)` method selects the rows at the specified indices, providing us with a subset of binding molecule samples. This subset will be used in our subsequent analysis and model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bind_table = scanner_protein_bind.take(indices=bind_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we perform a similar process to sample a specified number of non-binding molecule samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_bind_table = scanner_protein_no_bind.take(\n",
    "    indices=np.random.choice(n_rows_no_bind, size=n_no_bind, replace=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have sampled the specified number of binding and non-binding molecule samples, the next step is to combine these subsets into a single dataset.\n",
    "This is achieved by concatenating the tables containing the sampled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pa.concat_tables([bind_table, no_bind_table])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By concatenating the tables, we merge the subsets of binding and non-binding samples into a single, unified dataset.\n",
    "\n",
    "Most machine learning algorithms expect a single input dataset for training and evaluation. By concatenating the binding and non-binding samples into one table, we prepare the data in a format that is ready to be fed into machine learning models. This unified dataset can then be split into training and testing sets, features can be extracted, and models can be trained without additional steps to merge data.\n",
    "\n",
    "Having a single table that contains all the samples makes it easier to handle the data. We can apply transformations, feature extraction, and other preprocessing steps uniformly across the entire dataset without needing to manage multiple tables separately. This simplifies the workflow and reduces the potential for errors or inconsistencies in data processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features\n",
    "\n",
    "TODO: We define functions for cleaning the molecule string and extracting features using RDKit. The features are generated using Morgan fingerprints, a common method for encoding molecular structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'selfies'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrdkit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Chem\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrdkit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mChem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AllChem\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mleash_bio_kaggle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmol\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clean_mol_str\n",
      "File \u001b[0;32m~/leash-bio-kaggle/leash_bio_kaggle/mol.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Literal\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mselfies\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrdkit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Chem\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrdkit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mChem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AllChem\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'selfies'"
     ]
    }
   ],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from leash_bio_kaggle.mol import clean_mol_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(smiles: str, radius: int = 3, nBits: int = 2048):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    features = AllChem.GetMorganFingerprintAsBitVect(mol, radius=radius, nBits=nBits)\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row(row):\n",
    "    smiles = row['molecule_smiles']\n",
    "    fingerprint = get_features(clean_mol_str(smiles))\n",
    "    return fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_table_into_batches(table, batch_size):\n",
    "    num_rows = len(table)\n",
    "    for i in range(0, num_rows, batch_size):\n",
    "        yield table.slice(i, min(batch_size, num_rows - i))\n",
    "\n",
    "\n",
    "def generate_features_parallel(table, num_workers=4, batch_size=1000):\n",
    "    batches = split_table_into_batches(table, batch_size)\n",
    "    all_features = []\n",
    "\n",
    "    for batch in batches:\n",
    "        with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "            batch_dict = batch.to_pydict()\n",
    "            futures = [executor.submit(process_row, {key: batch_dict[key][i] for key in batch_dict}) for i in range(len(batch))]\n",
    "            batch_features = np.array([future.result() for future in futures])\n",
    "            all_features.append(batch_features)\n",
    "\n",
    "    features = np.vstack(all_features)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clean_mol_str' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m features_array \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_features_parallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 15\u001b[0m, in \u001b[0;36mgenerate_features_parallel\u001b[0;34m(table, num_workers, batch_size)\u001b[0m\n\u001b[1;32m     13\u001b[0m         batch_dict \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mto_pydict()\n\u001b[1;32m     14\u001b[0m         futures \u001b[38;5;241m=\u001b[39m [executor\u001b[38;5;241m.\u001b[39msubmit(process_row, {key: batch_dict[key][i] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m batch_dict}) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(batch))]\n\u001b[0;32m---> 15\u001b[0m         batch_features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[43m[\u001b[49m\u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfuture\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     16\u001b[0m         all_features\u001b[38;5;241m.\u001b[39mappend(batch_features)\n\u001b[1;32m     18\u001b[0m features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack(all_features)\n",
      "Cell \u001b[0;32mIn[26], line 15\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     13\u001b[0m         batch_dict \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mto_pydict()\n\u001b[1;32m     14\u001b[0m         futures \u001b[38;5;241m=\u001b[39m [executor\u001b[38;5;241m.\u001b[39msubmit(process_row, {key: batch_dict[key][i] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m batch_dict}) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(batch))]\n\u001b[0;32m---> 15\u001b[0m         batch_features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m futures])\n\u001b[1;32m     16\u001b[0m         all_features\u001b[38;5;241m.\u001b[39mappend(batch_features)\n\u001b[1;32m     18\u001b[0m features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack(all_features)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/leash_bio_kaggle-dev/lib/python3.11/concurrent/futures/_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/leash_bio_kaggle-dev/lib/python3.11/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/leash_bio_kaggle-dev/lib/python3.11/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "Cell \u001b[0;32mIn[22], line 3\u001b[0m, in \u001b[0;36mprocess_row\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_row\u001b[39m(row):\n\u001b[1;32m      2\u001b[0m     smiles \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmolecule_smiles\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m     fingerprint \u001b[38;5;241m=\u001b[39m get_features(\u001b[43mclean_mol_str\u001b[49m(smiles))\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fingerprint\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clean_mol_str' is not defined"
     ]
    }
   ],
   "source": [
    "features_array = generate_features_parallel(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biosc1540-2024s-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
