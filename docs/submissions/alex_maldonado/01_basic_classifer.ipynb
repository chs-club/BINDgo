{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 Basic classifier\n",
    "\n",
    "Here is a starter Jupyter notebook to get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration\n",
    "\n",
    "Data exploration is a crucial first step in any data science project.\n",
    "It involves understanding the data's structure, identifying patterns or trends, and detecting anomalies or missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data\n",
    "\n",
    "Typically, you can load a dataset using various methods in [Polars](https://docs.pola.rs/), such as normal loading and memory-mapped loading.\n",
    "These methods work well for smaller datasets but can cause memory issues with very large datasets, leading to kernel crashes.\n",
    "In such cases, chunked reading is a more efficient approach.\n",
    "\n",
    "Normal loading reads the entire dataset into memory at once.\n",
    "This method is straightforward but can be problematic with large datasets.\n",
    "\n",
    "```python\n",
    "import polars as pl\n",
    "\n",
    "# Load the dataset normally; however, this may crash for large datasets.\n",
    "data = pl.read_parquet(\"../../../data/train.parquet\")\n",
    "```\n",
    "\n",
    "Memory-mapped loading can improve performance by mapping the file directly into memory, reducing the overhead of data copying.\n",
    "However, it still requires sufficient memory to hold the entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunked reading\n",
    "\n",
    "For very large datasets, chunked reading is a more efficient approach.\n",
    "This method reads the dataset in smaller chunks, which helps manage memory usage more effectively.\n",
    "By processing data incrementally, chunked reading avoids the pitfalls of memory overload and allows for scalable data exploration and analysis.\n",
    "\n",
    "1. **Memory Management**: Large datasets can easily exceed the available memory, causing kernel crashes or significant slowdowns. Chunked reading mitigates this by only loading manageable portions of the data at a time.\n",
    "2. **Scalability**: Chunked reading enables the handling of datasets that are much larger than the systemâ€™s memory capacity, making it a scalable solution for big data problems.\n",
    "3. **Incremental Processing**: It allows for incremental processing of data, which can be useful for real-time data analysis and processing tasks.\n",
    "4. **Flexibility**: You can perform operations on each chunk independently, which provides flexibility in data processing and can lead to more efficient computations.\n",
    "\n",
    "Chunked reading involves reading a fixed number of rows (a chunk) from the dataset, processing that chunk, and then moving on to the next chunk.\n",
    "This approach ensures that only a small portion of the dataset is loaded into memory at any given time.\n",
    "To start, we need to import the necessary library for reading parquet files.\n",
    "We will use the [`pyarrow.parquet`](https://arrow.apache.org/docs/python/index.html) library to handle the parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the library imported, we can now define a function to read the dataset in chunks.\n",
    "This will help us manage the large data size without overwhelming our system's memory.\n",
    "\n",
    "\n",
    "[ParqetFile](https://arrow.apache.org/docs/python/generated/pyarrow.parquet.ParquetFile.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train_data = \"../../../data/train.parquet\"\n",
    "data_train = pq.ParquetFile(source=path_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For very large datasets, using `pyarrow.ParquetFile` with the `iter_batches` method is an efficient approach to manage memory usage.\n",
    "This method reads the dataset in smaller chunks or batches, allowing for scalable data exploration and analysis.\n",
    "We will define a function to read the dataset in batches using `pyarrow`.\n",
    "This function will handle reading the data in specified batch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_gen = data_train.iter_batches()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure our batch reading function works correctly, we will retrieve and inspect the first batch of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyarrow.RecordBatch\n",
      "id: int64\n",
      "buildingblock1_smiles: string\n",
      "buildingblock2_smiles: string\n",
      "buildingblock3_smiles: string\n",
      "molecule_smiles: string\n",
      "protein_name: string\n",
      "binds: int64\n",
      "----\n",
      "id: [0,1,2,3,4,5,6,7,8,9,...,65526,65527,65528,65529,65530,65531,65532,65533,65534,65535]\n",
      "buildingblock1_smiles: [\"C#CC[C@@H](CC(=O)O)NC(=O)OCC1c2ccccc2-c2ccccc21\",\"C#CC[C@@H](CC(=O)O)NC(=O)OCC1c2ccccc2-c2ccccc21\",\"C#CC[C@@H](CC(=O)O)NC(=O)OCC1c2ccccc2-c2ccccc21\",\"C#CC[C@@H](CC(=O)O)NC(=O)OCC1c2ccccc2-c2ccccc21\",\"C#CC[C@@H](CC(=O)O)NC(=O)OCC1c2ccccc2-c2ccccc21\",\"C#CC[C@@H](CC(=O)O)NC(=O)OCC1c2ccccc2-c2ccccc21\",\"C#CC[C@@H](CC(=O)O)NC(=O)OCC1c2ccccc2-c2ccccc21\",\"C#CC[C@@H](CC(=O)O)NC(=O)OCC1c2ccccc2-c2ccccc21\",\"C#CC[C@@H](CC(=O)O)NC(=O)OCC1c2ccccc2-c2ccccc21\",\"C#CC[C@@H](CC(=O)O)NC(=O)OCC1c2ccccc2-c2ccccc21\",...,\"C#CC[C@@H](CC(=O)O)NC(=O)OCC1c2ccccc2-c2ccccc21\",\"C#CC[C@@H](CC(=O)O)NC(=O)OCC1c2ccccc2-c2ccccc21\",\"C#CC[C@@H](CC(=O)O)NC(=O)OCC1c2ccccc2-c2ccccc21\",\"C#CC[C@@H](CC(=O)O)NC(=O)OCC1c2ccccc2-c2ccccc21\",\"C#CC[C@@H](CC(=O)O)NC(=O)OCC1c2ccccc2-c2ccccc21\",\"C#CC[C@@H](CC(=O)O)NC(=O)OCC1c2ccccc2-c2ccccc21\",\"C#CC[C@@H](CC(=O)O)NC(=O)OCC1c2ccccc2-c2ccccc21\",\"C#CC[C@@H](CC(=O)O)NC(=O)OCC1c2ccccc2-c2ccccc21\",\"C#CC[C@@H](CC(=O)O)NC(=O)OCC1c2ccccc2-c2ccccc21\",\"C#CC[C@@H](CC(=O)O)NC(=O)OCC1c2ccccc2-c2ccccc21\"]\n",
      "buildingblock2_smiles: [\"C#CCOc1ccc(CN)cc1.Cl\",\"C#CCOc1ccc(CN)cc1.Cl\",\"C#CCOc1ccc(CN)cc1.Cl\",\"C#CCOc1ccc(CN)cc1.Cl\",\"C#CCOc1ccc(CN)cc1.Cl\",\"C#CCOc1ccc(CN)cc1.Cl\",\"C#CCOc1ccc(CN)cc1.Cl\",\"C#CCOc1ccc(CN)cc1.Cl\",\"C#CCOc1ccc(CN)cc1.Cl\",\"C#CCOc1ccc(CN)cc1.Cl\",...,\"CC(F)(F)CN.Cl\",\"CC(F)(F)CN.Cl\",\"CC(F)(F)CN.Cl\",\"CC(F)(F)CN.Cl\",\"CC(F)(F)CN.Cl\",\"CC(F)(F)CN.Cl\",\"CC(F)(F)CN.Cl\",\"CC(F)(F)CN.Cl\",\"CC(F)(F)CN.Cl\",\"CC(F)(F)CN.Cl\"]\n",
      "buildingblock3_smiles: [\"Br.Br.NCC1CCCN1c1cccnn1\",\"Br.Br.NCC1CCCN1c1cccnn1\",\"Br.Br.NCC1CCCN1c1cccnn1\",\"Br.NCc1cccc(Br)n1\",\"Br.NCc1cccc(Br)n1\",\"Br.NCc1cccc(Br)n1\",\"C#CCOc1ccc(CN)cc1.Cl\",\"C#CCOc1ccc(CN)cc1.Cl\",\"C#CCOc1ccc(CN)cc1.Cl\",\"C=C(C)C(=O)NCCN.Cl\",...,\"Nc1cc(-c2cccc(Br)c2)no1\",\"Nc1cc(-c2cccc(Br)c2)no1\",\"Nc1cc(-c2cccc(Br)c2)no1\",\"Nc1cc(-c2ccccc2)[nH]n1\",\"Nc1cc(-c2ccccc2)[nH]n1\",\"Nc1cc(-c2ccccc2)[nH]n1\",\"Nc1cc(=O)[nH]c(=O)[nH]1\",\"Nc1cc(=O)[nH]c(=O)[nH]1\",\"Nc1cc(=O)[nH]c(=O)[nH]1\",\"Nc1cc(=O)[nH]c(=S)[nH]1\"]\n",
      "molecule_smiles: [\"C#CCOc1ccc(CNc2nc(NCC3CCCN3c3cccnn3)nc(N[C@@H](CC#C)CC(=O)N[Dy])n2)cc1\",\"C#CCOc1ccc(CNc2nc(NCC3CCCN3c3cccnn3)nc(N[C@@H](CC#C)CC(=O)N[Dy])n2)cc1\",\"C#CCOc1ccc(CNc2nc(NCC3CCCN3c3cccnn3)nc(N[C@@H](CC#C)CC(=O)N[Dy])n2)cc1\",\"C#CCOc1ccc(CNc2nc(NCc3cccc(Br)n3)nc(N[C@@H](CC#C)CC(=O)N[Dy])n2)cc1\",\"C#CCOc1ccc(CNc2nc(NCc3cccc(Br)n3)nc(N[C@@H](CC#C)CC(=O)N[Dy])n2)cc1\",\"C#CCOc1ccc(CNc2nc(NCc3cccc(Br)n3)nc(N[C@@H](CC#C)CC(=O)N[Dy])n2)cc1\",\"C#CCOc1ccc(CNc2nc(NCc3ccc(OCC#C)cc3)nc(N[C@@H](CC#C)CC(=O)N[Dy])n2)cc1\",\"C#CCOc1ccc(CNc2nc(NCc3ccc(OCC#C)cc3)nc(N[C@@H](CC#C)CC(=O)N[Dy])n2)cc1\",\"C#CCOc1ccc(CNc2nc(NCc3ccc(OCC#C)cc3)nc(N[C@@H](CC#C)CC(=O)N[Dy])n2)cc1\",\"C#CCOc1ccc(CNc2nc(NCCNC(=O)C(=C)C)nc(N[C@@H](CC#C)CC(=O)N[Dy])n2)cc1\",...,\"C#CC[C@@H](CC(=O)N[Dy])Nc1nc(NCC(C)(F)F)nc(Nc2cc(-c3cccc(Br)c3)no2)n1\",\"C#CC[C@@H](CC(=O)N[Dy])Nc1nc(NCC(C)(F)F)nc(Nc2cc(-c3cccc(Br)c3)no2)n1\",\"C#CC[C@@H](CC(=O)N[Dy])Nc1nc(NCC(C)(F)F)nc(Nc2cc(-c3cccc(Br)c3)no2)n1\",\"C#CC[C@@H](CC(=O)N[Dy])Nc1nc(NCC(C)(F)F)nc(Nc2cc(-c3ccccc3)[nH]n2)n1\",\"C#CC[C@@H](CC(=O)N[Dy])Nc1nc(NCC(C)(F)F)nc(Nc2cc(-c3ccccc3)[nH]n2)n1\",\"C#CC[C@@H](CC(=O)N[Dy])Nc1nc(NCC(C)(F)F)nc(Nc2cc(-c3ccccc3)[nH]n2)n1\",\"C#CC[C@@H](CC(=O)N[Dy])Nc1nc(NCC(C)(F)F)nc(Nc2cc(=O)[nH]c(=O)[nH]2)n1\",\"C#CC[C@@H](CC(=O)N[Dy])Nc1nc(NCC(C)(F)F)nc(Nc2cc(=O)[nH]c(=O)[nH]2)n1\",\"C#CC[C@@H](CC(=O)N[Dy])Nc1nc(NCC(C)(F)F)nc(Nc2cc(=O)[nH]c(=O)[nH]2)n1\",\"C#CC[C@@H](CC(=O)N[Dy])Nc1nc(NCC(C)(F)F)nc(Nc2cc(=O)[nH]c(=S)[nH]2)n1\"]\n",
      "protein_name: [\"BRD4\",\"HSA\",\"sEH\",\"BRD4\",\"HSA\",\"sEH\",\"BRD4\",\"HSA\",\"sEH\",\"BRD4\",...,\"BRD4\",\"HSA\",\"sEH\",\"BRD4\",\"HSA\",\"sEH\",\"BRD4\",\"HSA\",\"sEH\",\"BRD4\"]\n",
      "binds: [0,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,0]\n"
     ]
    }
   ],
   "source": [
    "data_train_sample = next(data_train.iter_batches())\n",
    "print(data_train_sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biosc1540-2024s-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
