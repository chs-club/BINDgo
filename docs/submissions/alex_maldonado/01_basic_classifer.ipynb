{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 Basic classifier\n",
    "\n",
    "Here is a starter Jupyter notebook to get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration\n",
    "\n",
    "Data exploration is a crucial first step in any data science project.\n",
    "It involves understanding the data's structure, identifying patterns or trends, and detecting anomalies or missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data\n",
    "\n",
    "Loading data is a fundamental step in any data science project, setting the stage for all subsequent analysis and modeling tasks.\n",
    "Efficiently managing and loading data is especially critical when dealing with large datasets, as improper handling can lead to memory issues and slow performance.\n",
    "In this section, we will demonstrate how to load our dataset using the [`polars`](https://docs.pola.rs/) and [`pyarrow`](https://arrow.apache.org/docs/python/).\n",
    "\n",
    "To begin with, we import the [`polars`](https://docs.pola.rs/) library using `import polars as pl`; this high-performance DataFrame library is designed to handle large datasets efficiently.\n",
    "Polars is known for its speed and memory efficiency, making it an excellent choice for data manipulation and analytical tasks where performance is crucial.\n",
    "It offers a familiar `DataFrame` API that is intuitive for users with experience in [`pandas`](https://pandas.pydata.org/) but provides significant performance advantages, particularly with large datasets.\n",
    "Using Polars ensures that our data processing tasks remain performant and scalable, preventing the crashes and slowdowns that often occur with less efficient libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While `polars` is designed for high-performance data set analysis, there are always limits from the data set size and computational hardware&mdash;specifically memory.\n",
    "Our Leash biosciences training data is a 3.8 GB [parquet](https://parquet.apache.org/docs/) file which could likely cause issues if trying to load it with `polars`.\n",
    "\n",
    "Instead, we will demonstrate how to load parquet files with the test set (which is only 30.2 MB).\n",
    "This is done with the [`read_parquet`](https://docs.pola.rs/py-polars/html/reference/api/polars.read_parquet.html) function in polars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pl.read_parquet(source=\"../../../data/test.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is loaded into the DataFrame, we can inspect the first few rows to ensure it has been read correctly and to get an initial understanding of its structure.\n",
    "This initial inspection helps with:\n",
    "\n",
    "- **Data Verification**: Ensuring that the data has been loaded correctly is a critical first step. By inspecting the first few rows, we confirm that the data structure matches our expectations.\n",
    "- **Understanding Structure**: This initial inspection provides a clear picture of the data's structure, which is essential for planning subsequent data processing and analysis steps.\n",
    "- **Spotting Issues Early**: Early detection of any anomalies or inconsistencies in the data can save significant time and effort later in the analysis process.\n",
    "\n",
    "The [`DataFrame.head`](https://docs.pola.rs/py-polars/html/reference/dataframe/api/polars.DataFrame.head.html#polars.DataFrame.head) function will print the first five of the DataFrame, giving us a snapshot of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 6)\n",
      "┌───────────┬─────────────────┬─────────────────┬─────────────────┬─────────────────┬──────────────┐\n",
      "│ id        ┆ buildingblock1_ ┆ buildingblock2_ ┆ buildingblock3_ ┆ molecule_smiles ┆ protein_name │\n",
      "│ ---       ┆ smiles          ┆ smiles          ┆ smiles          ┆ ---             ┆ ---          │\n",
      "│ i64       ┆ ---             ┆ ---             ┆ ---             ┆ str             ┆ str          │\n",
      "│           ┆ str             ┆ str             ┆ str             ┆                 ┆              │\n",
      "╞═══════════╪═════════════════╪═════════════════╪═════════════════╪═════════════════╪══════════════╡\n",
      "│ 295246830 ┆ C#CCCC[C@H](NC( ┆ C=Cc1ccc(N)cc1  ┆ C=Cc1ccc(N)cc1  ┆ C#CCCC[C@H](Nc1 ┆ BRD4         │\n",
      "│           ┆ =O)OCC1c2ccccc2 ┆                 ┆                 ┆ nc(Nc2ccc(C=C)c ┆              │\n",
      "│           ┆ …               ┆                 ┆                 ┆ …               ┆              │\n",
      "│ 295246831 ┆ C#CCCC[C@H](NC( ┆ C=Cc1ccc(N)cc1  ┆ C=Cc1ccc(N)cc1  ┆ C#CCCC[C@H](Nc1 ┆ HSA          │\n",
      "│           ┆ =O)OCC1c2ccccc2 ┆                 ┆                 ┆ nc(Nc2ccc(C=C)c ┆              │\n",
      "│           ┆ …               ┆                 ┆                 ┆ …               ┆              │\n",
      "│ 295246832 ┆ C#CCCC[C@H](NC( ┆ C=Cc1ccc(N)cc1  ┆ C=Cc1ccc(N)cc1  ┆ C#CCCC[C@H](Nc1 ┆ sEH          │\n",
      "│           ┆ =O)OCC1c2ccccc2 ┆                 ┆                 ┆ nc(Nc2ccc(C=C)c ┆              │\n",
      "│           ┆ …               ┆                 ┆                 ┆ …               ┆              │\n",
      "│ 295246833 ┆ C#CCCC[C@H](NC( ┆ C=Cc1ccc(N)cc1  ┆ CC(O)Cn1cnc2c(N ┆ C#CCCC[C@H](Nc1 ┆ BRD4         │\n",
      "│           ┆ =O)OCC1c2ccccc2 ┆                 ┆ )ncnc21         ┆ nc(Nc2ccc(C=C)c ┆              │\n",
      "│           ┆ …               ┆                 ┆                 ┆ …               ┆              │\n",
      "│ 295246834 ┆ C#CCCC[C@H](NC( ┆ C=Cc1ccc(N)cc1  ┆ CC(O)Cn1cnc2c(N ┆ C#CCCC[C@H](Nc1 ┆ HSA          │\n",
      "│           ┆ =O)OCC1c2ccccc2 ┆                 ┆ )ncnc21         ┆ nc(Nc2ccc(C=C)c ┆              │\n",
      "│           ┆ …               ┆                 ┆                 ┆ …               ┆              │\n",
      "└───────────┴─────────────────┴─────────────────┴─────────────────┴─────────────────┴──────────────┘\n"
     ]
    }
   ],
   "source": [
    "print(data_test.head(n=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the test data into a Polars DataFrame, we inspect the first few rows to verify the data's integrity and understand its structure.\n",
    "The output reveals that the DataFrame has a shape of 5 rows and 6 columns, which aligns with our request to display the first five rows.\n",
    "This initial check confirms that the DataFrame has the expected dimensions, which is a critical step in validating that the data has been loaded correctly.\n",
    "\n",
    "Each column in the DataFrame serves a specific purpose and has a designated data type.\n",
    "The `id` column is a unique identifier for each molecule-protein pair, and it is stored as a 64-bit integer (`i64`).\n",
    "The `buildingblock1_smiles`, `buildingblock2_smiles`, and `buildingblock3_smiles` columns contain the SMILES representations of the individual building blocks of the molecules.\n",
    "These columns are of type `str` (string), as SMILES strings are textual representations of chemical structures.\n",
    "The `molecule_smiles` column holds the SMILES representation of the fully assembled molecule, which includes the three building blocks and the core structure.\n",
    "This column is also of type `str`. Lastly, the `protein_name` column, which is of type `str`, indicates the name of the protein target that each molecule is associated with.\n",
    "\n",
    "The content of the rows provides a snapshot of the data, where each row represents a unique molecule-protein pair along with its corresponding SMILES strings and protein name.\n",
    "For instance, the first row shows a molecule with an `id` of 295246830, which is composed of three building blocks with their respective SMILES strings.\n",
    "The fully assembled molecule's SMILES string is also provided, and the protein target for this molecule is `BRD4`. This detailed breakdown allows us to understand the composition and target of each molecule in the dataset.\n",
    "\n",
    "Additionally, we notice the presence of ellipses (`…`) in the SMILES strings.\n",
    "These ellipses indicate that the SMILES strings are too long to be fully displayed in the output, which is common given the complexity and length of chemical structures represented in SMILES notation.\n",
    "The truncated display helps maintain readability while providing a glimpse of the data's content.\n",
    "\n",
    "We can also look at the last two rows using the [`DataFrame.tail`](https://docs.pola.rs/py-polars/html/reference/dataframe/api/polars.DataFrame.tail.html) function.\n",
    "Now we have to specify `n=2` to change the number of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (2, 6)\n",
      "┌───────────┬─────────────────┬─────────────────┬─────────────────┬─────────────────┬──────────────┐\n",
      "│ id        ┆ buildingblock1_ ┆ buildingblock2_ ┆ buildingblock3_ ┆ molecule_smiles ┆ protein_name │\n",
      "│ ---       ┆ smiles          ┆ smiles          ┆ smiles          ┆ ---             ┆ ---          │\n",
      "│ i64       ┆ ---             ┆ ---             ┆ ---             ┆ str             ┆ str          │\n",
      "│           ┆ str             ┆ str             ┆ str             ┆                 ┆              │\n",
      "╞═══════════╪═════════════════╪═════════════════╪═════════════════╪═════════════════╪══════════════╡\n",
      "│ 296921724 ┆ [N-]=[N+]=NCCC[ ┆ Nc1noc2ccc(F)cc ┆ NCc1cccs1       ┆ [N-]=[N+]=NCCC[ ┆ HSA          │\n",
      "│           ┆ C@H](NC(=O)OCC1 ┆ 12              ┆                 ┆ C@H](Nc1nc(NCc2 ┆              │\n",
      "│           ┆ …               ┆                 ┆                 ┆ …               ┆              │\n",
      "│ 296921725 ┆ [N-]=[N+]=NCCC[ ┆ Nc1noc2ccc(F)cc ┆ NCc1cccs1       ┆ [N-]=[N+]=NCCC[ ┆ sEH          │\n",
      "│           ┆ C@H](NC(=O)OCC1 ┆ 12              ┆                 ┆ C@H](Nc1nc(NCc2 ┆              │\n",
      "│           ┆ …               ┆                 ┆                 ┆ …               ┆              │\n",
      "└───────────┴─────────────────┴─────────────────┴─────────────────┴─────────────────┴──────────────┘\n"
     ]
    }
   ],
   "source": [
    "print(data_test.tail(n=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunked reading\n",
    "\n",
    "For very large datasets, chunked reading is a more efficient approach.\n",
    "This method reads the dataset in smaller chunks, which helps manage memory usage more effectively.\n",
    "By processing data incrementally, chunked reading avoids the pitfalls of memory overload and allows for scalable data exploration and analysis.\n",
    "\n",
    "1. **Memory Management**: Large datasets can easily exceed the available memory, causing kernel crashes or significant slowdowns. Chunked reading mitigates this by only loading manageable portions of the data at a time.\n",
    "2. **Scalability**: Chunked reading enables the handling of datasets that are much larger than the system’s memory capacity, making it a scalable solution for big data problems.\n",
    "3. **Incremental Processing**: It allows for incremental processing of data, which can be useful for real-time data analysis and processing tasks.\n",
    "4. **Flexibility**: You can perform operations on each chunk independently, which provides flexibility in data processing and can lead to more efficient computations.\n",
    "\n",
    "Chunked reading involves reading a fixed number of rows (a chunk) from the dataset, processing that chunk, and then moving on to the next chunk.\n",
    "This approach ensures that only a small portion of the dataset is loaded into memory at any given time.\n",
    "To start, we need to import the necessary library for reading parquet files.\n",
    "We will use the [`pyarrow.parquet`](https://arrow.apache.org/docs/python/index.html) library to handle the parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the library imported, we can now define a function to read the dataset in chunks.\n",
    "This will help us manage the large data size without overwhelming our system's memory.\n",
    "\n",
    "\n",
    "[ParqetFile](https://arrow.apache.org/docs/python/generated/pyarrow.parquet.ParquetFile.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train_data = \"../../../data/train.parquet\"\n",
    "data_train = pq.ParquetFile(source=path_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For very large datasets, using `pyarrow.ParquetFile` with the `iter_batches` method is an efficient approach to manage memory usage.\n",
    "This method reads the dataset in smaller chunks or batches, allowing for scalable data exploration and analysis.\n",
    "We will define a function to read the dataset in batches using `pyarrow`.\n",
    "This function will handle reading the data in specified batch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_gen = data_train.iter_batches()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure our batch reading function works correctly, we will retrieve and inspect the first batch of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyarrow.RecordBatch\n",
      "id: int64\n",
      "buildingblock1_smiles: string\n",
      "buildingblock2_smiles: string\n",
      "buildingblock3_smiles: string\n",
      "molecule_smiles: string\n",
      "protein_name: string\n",
      "binds: int64\n",
      "----\n",
      "id: [0,1,2,3,4,5,6,7,8,9,...,65526,65527,65528,65529,65530,65531,65532,65533,65534,65535]\n",
      "buildingblock1_smiles: [\"C#CC[C@@H](CC(=O)O)NC(=O)OCC1c2ccccc2-c2ccccc21\",\"C#CC[C@@H](CC(=O)O)NC(=O)OCC1c2ccccc2-c2ccccc21\",\"C#CC[C@@H](CC(=O)O)NC(=O)OCC1c2ccccc2-c2ccccc21\",\"C#CC[C@@H](CC(=O)O)NC(=O)OCC1c2ccccc2-c2ccccc21\",\"C#CC[C@@H](CC(=O)O)NC(=O)OCC1c2ccccc2-c2ccccc21\",\"C#CC[C@@H](CC(=O)O)NC(=O)OCC1c2ccccc2-c2ccccc21\",\"C#CC[C@@H](CC(=O)O)NC(=O)OCC1c2ccccc2-c2ccccc21\",\"C#CC[C@@H](CC(=O)O)NC(=O)OCC1c2ccccc2-c2ccccc21\",\"C#CC[C@@H](CC(=O)O)NC(=O)OCC1c2ccccc2-c2ccccc21\",\"C#CC[C@@H](CC(=O)O)NC(=O)OCC1c2ccccc2-c2ccccc21\",...,\"C#CC[C@@H](CC(=O)O)NC(=O)OCC1c2ccccc2-c2ccccc21\",\"C#CC[C@@H](CC(=O)O)NC(=O)OCC1c2ccccc2-c2ccccc21\",\"C#CC[C@@H](CC(=O)O)NC(=O)OCC1c2ccccc2-c2ccccc21\",\"C#CC[C@@H](CC(=O)O)NC(=O)OCC1c2ccccc2-c2ccccc21\",\"C#CC[C@@H](CC(=O)O)NC(=O)OCC1c2ccccc2-c2ccccc21\",\"C#CC[C@@H](CC(=O)O)NC(=O)OCC1c2ccccc2-c2ccccc21\",\"C#CC[C@@H](CC(=O)O)NC(=O)OCC1c2ccccc2-c2ccccc21\",\"C#CC[C@@H](CC(=O)O)NC(=O)OCC1c2ccccc2-c2ccccc21\",\"C#CC[C@@H](CC(=O)O)NC(=O)OCC1c2ccccc2-c2ccccc21\",\"C#CC[C@@H](CC(=O)O)NC(=O)OCC1c2ccccc2-c2ccccc21\"]\n",
      "buildingblock2_smiles: [\"C#CCOc1ccc(CN)cc1.Cl\",\"C#CCOc1ccc(CN)cc1.Cl\",\"C#CCOc1ccc(CN)cc1.Cl\",\"C#CCOc1ccc(CN)cc1.Cl\",\"C#CCOc1ccc(CN)cc1.Cl\",\"C#CCOc1ccc(CN)cc1.Cl\",\"C#CCOc1ccc(CN)cc1.Cl\",\"C#CCOc1ccc(CN)cc1.Cl\",\"C#CCOc1ccc(CN)cc1.Cl\",\"C#CCOc1ccc(CN)cc1.Cl\",...,\"CC(F)(F)CN.Cl\",\"CC(F)(F)CN.Cl\",\"CC(F)(F)CN.Cl\",\"CC(F)(F)CN.Cl\",\"CC(F)(F)CN.Cl\",\"CC(F)(F)CN.Cl\",\"CC(F)(F)CN.Cl\",\"CC(F)(F)CN.Cl\",\"CC(F)(F)CN.Cl\",\"CC(F)(F)CN.Cl\"]\n",
      "buildingblock3_smiles: [\"Br.Br.NCC1CCCN1c1cccnn1\",\"Br.Br.NCC1CCCN1c1cccnn1\",\"Br.Br.NCC1CCCN1c1cccnn1\",\"Br.NCc1cccc(Br)n1\",\"Br.NCc1cccc(Br)n1\",\"Br.NCc1cccc(Br)n1\",\"C#CCOc1ccc(CN)cc1.Cl\",\"C#CCOc1ccc(CN)cc1.Cl\",\"C#CCOc1ccc(CN)cc1.Cl\",\"C=C(C)C(=O)NCCN.Cl\",...,\"Nc1cc(-c2cccc(Br)c2)no1\",\"Nc1cc(-c2cccc(Br)c2)no1\",\"Nc1cc(-c2cccc(Br)c2)no1\",\"Nc1cc(-c2ccccc2)[nH]n1\",\"Nc1cc(-c2ccccc2)[nH]n1\",\"Nc1cc(-c2ccccc2)[nH]n1\",\"Nc1cc(=O)[nH]c(=O)[nH]1\",\"Nc1cc(=O)[nH]c(=O)[nH]1\",\"Nc1cc(=O)[nH]c(=O)[nH]1\",\"Nc1cc(=O)[nH]c(=S)[nH]1\"]\n",
      "molecule_smiles: [\"C#CCOc1ccc(CNc2nc(NCC3CCCN3c3cccnn3)nc(N[C@@H](CC#C)CC(=O)N[Dy])n2)cc1\",\"C#CCOc1ccc(CNc2nc(NCC3CCCN3c3cccnn3)nc(N[C@@H](CC#C)CC(=O)N[Dy])n2)cc1\",\"C#CCOc1ccc(CNc2nc(NCC3CCCN3c3cccnn3)nc(N[C@@H](CC#C)CC(=O)N[Dy])n2)cc1\",\"C#CCOc1ccc(CNc2nc(NCc3cccc(Br)n3)nc(N[C@@H](CC#C)CC(=O)N[Dy])n2)cc1\",\"C#CCOc1ccc(CNc2nc(NCc3cccc(Br)n3)nc(N[C@@H](CC#C)CC(=O)N[Dy])n2)cc1\",\"C#CCOc1ccc(CNc2nc(NCc3cccc(Br)n3)nc(N[C@@H](CC#C)CC(=O)N[Dy])n2)cc1\",\"C#CCOc1ccc(CNc2nc(NCc3ccc(OCC#C)cc3)nc(N[C@@H](CC#C)CC(=O)N[Dy])n2)cc1\",\"C#CCOc1ccc(CNc2nc(NCc3ccc(OCC#C)cc3)nc(N[C@@H](CC#C)CC(=O)N[Dy])n2)cc1\",\"C#CCOc1ccc(CNc2nc(NCc3ccc(OCC#C)cc3)nc(N[C@@H](CC#C)CC(=O)N[Dy])n2)cc1\",\"C#CCOc1ccc(CNc2nc(NCCNC(=O)C(=C)C)nc(N[C@@H](CC#C)CC(=O)N[Dy])n2)cc1\",...,\"C#CC[C@@H](CC(=O)N[Dy])Nc1nc(NCC(C)(F)F)nc(Nc2cc(-c3cccc(Br)c3)no2)n1\",\"C#CC[C@@H](CC(=O)N[Dy])Nc1nc(NCC(C)(F)F)nc(Nc2cc(-c3cccc(Br)c3)no2)n1\",\"C#CC[C@@H](CC(=O)N[Dy])Nc1nc(NCC(C)(F)F)nc(Nc2cc(-c3cccc(Br)c3)no2)n1\",\"C#CC[C@@H](CC(=O)N[Dy])Nc1nc(NCC(C)(F)F)nc(Nc2cc(-c3ccccc3)[nH]n2)n1\",\"C#CC[C@@H](CC(=O)N[Dy])Nc1nc(NCC(C)(F)F)nc(Nc2cc(-c3ccccc3)[nH]n2)n1\",\"C#CC[C@@H](CC(=O)N[Dy])Nc1nc(NCC(C)(F)F)nc(Nc2cc(-c3ccccc3)[nH]n2)n1\",\"C#CC[C@@H](CC(=O)N[Dy])Nc1nc(NCC(C)(F)F)nc(Nc2cc(=O)[nH]c(=O)[nH]2)n1\",\"C#CC[C@@H](CC(=O)N[Dy])Nc1nc(NCC(C)(F)F)nc(Nc2cc(=O)[nH]c(=O)[nH]2)n1\",\"C#CC[C@@H](CC(=O)N[Dy])Nc1nc(NCC(C)(F)F)nc(Nc2cc(=O)[nH]c(=O)[nH]2)n1\",\"C#CC[C@@H](CC(=O)N[Dy])Nc1nc(NCC(C)(F)F)nc(Nc2cc(=O)[nH]c(=S)[nH]2)n1\"]\n",
      "protein_name: [\"BRD4\",\"HSA\",\"sEH\",\"BRD4\",\"HSA\",\"sEH\",\"BRD4\",\"HSA\",\"sEH\",\"BRD4\",...,\"BRD4\",\"HSA\",\"sEH\",\"BRD4\",\"HSA\",\"sEH\",\"BRD4\",\"HSA\",\"sEH\",\"BRD4\"]\n",
      "binds: [0,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,0]\n"
     ]
    }
   ],
   "source": [
    "data_train_sample = next(data_train_gen)\n",
    "print(data_train_sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biosc1540-2024s-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
