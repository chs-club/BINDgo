{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 Basic classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and processing\n",
    "\n",
    "TODO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.dataset as ds\n",
    "import pyarrow.compute as pc\n",
    "\n",
    "PATH_TRAIN_DATA = \"../../../data/train.parquet\"\n",
    "PATH_TEST_DATA = \"../../../data/test.parquet\"\n",
    "DATA = ds.dataset(source=PATH_TRAIN_DATA, format=\"parquet\")\n",
    "DATA_TEST = ds.dataset(source=PATH_TEST_DATA, format=\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scanner_no_bind = DATA.scanner(filter=(pc.field(\"binds\") == 0))\n",
    "scanner_bind = DATA.scanner(filter=(pc.field(\"binds\") == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.typing as npt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_indices(\n",
    "    n_rows, train_split: float = 0.8, size: str | None = None\n",
    ") -> (npt.NDArray[np.uint64], npt.NDArray[np.uint64]):\n",
    "    \"\"\"\n",
    "    Splits the indices of rows into training and validation sets.\n",
    "\n",
    "    Args:\n",
    "        n_rows: The total number of rows to generate indices for.\n",
    "        train_split: The proportion of indices to allocate to the training set.\n",
    "        size: If provided, trims the number of indices to this size.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing two arrays:\n",
    "\n",
    "            - train_indices: Indices for the training set.\n",
    "            - val_indices: Indices for the validation set.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate indices and shuffle them in place.\n",
    "    indices = np.arange(n_rows)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    # Trim the number of indices to size.\n",
    "    indices = indices[None:size]\n",
    "\n",
    "    # Split indices into training and validation sets\n",
    "    train_size = int(indices.shape[0] * train_split)\n",
    "    train_indices = indices[:train_size]\n",
    "    val_indices = indices[train_size:]\n",
    "\n",
    "    return train_indices, val_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_NO_BIND = 293656924\n",
    "N_BIND = 1589906"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SPLIT = 0.8\n",
    "SIZE = 50000\n",
    "\n",
    "train_indices_no_bind, valid_indices_no_bind = split_indices(\n",
    "    N_NO_BIND, TRAIN_SPLIT, size=SIZE\n",
    ")\n",
    "train_indices_bind, valid_indices_bind = split_indices(\n",
    "    N_BIND, TRAIN_SPLIT, size=SIZE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features\n",
    "\n",
    "TODO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from concurrent.futures import ProcessPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_smiles(smiles):\n",
    "\n",
    "    # Remove [Dy] from smiles\n",
    "    smiles = smiles.replace(\"[Dy]\", \"\")\n",
    "\n",
    "    # Convert SMILES to a RDKit molecule object\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        raise ValueError(\"Invalid SMILES string\")\n",
    "    \n",
    "    # Remove any salts or fragments\n",
    "    mol = Chem.RemoveHs(mol)  # Remove explicit hydrogens\n",
    "    fragments = Chem.GetMolFrags(mol, asMols=True)\n",
    "    \n",
    "    # Keep the largest fragment\n",
    "    largest_fragment = max(fragments, default=mol, key=lambda m: m.GetNumAtoms())\n",
    "    \n",
    "    # Standardize the molecule\n",
    "    AllChem.Compute2DCoords(largest_fragment)  # Compute 2D coordinates\n",
    "    \n",
    "    # Convert the molecule back to a canonical SMILES string\n",
    "    cleaned_smiles = Chem.MolToSmiles(largest_fragment, canonical=True)\n",
    "    return cleaned_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mol(smiles, optimize=False):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    mol = Chem.AddHs(mol)\n",
    "    AllChem.EmbedMolecule(mol)\n",
    "    if optimize:\n",
    "        AllChem.MMFFOptimizeMolecule(mol, maxIters=200)\n",
    "    return mol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(smiles: str, radius=3, nBits=2048):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    features = AllChem.GetMorganFingerprintAsBitVect(mol, radius=radius, nBits=nBits)\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row(row):\n",
    "    smiles = row['molecule_smiles']\n",
    "    fingerprint = get_features(clean_smiles(smiles))\n",
    "    return fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batch):\n",
    "    fingerprints = []\n",
    "    for row in batch:\n",
    "        try:\n",
    "            fingerprint = process_row(row)\n",
    "            fingerprints.append(fingerprint)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return np.array(fingerprints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def compute_fingerprints(scanner, indices, chunk_size=1000):\n",
    "    def generator(chunk_indices):\n",
    "        for index in chunk_indices:\n",
    "            yield scanner.to_table().slice(index, 1).to_pandas().to_dict('records')[0]\n",
    "\n",
    "    fingerprints = []\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        for chunk_start in range(0, len(indices), chunk_size):\n",
    "            chunk_indices = indices[chunk_start:chunk_start + chunk_size]\n",
    "            chunks = [record for record in generator(chunk_indices)]\n",
    "            result_batches = executor.map(process_batch, np.array_split(chunks, 8))\n",
    "            fingerprints.extend(np.concatenate(list(result_batches)))\n",
    "    return np.array(fingerprints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_bind_fingerprints_train = compute_fingerprints(scanner_no_bind, train_indices_no_bind)\n",
    "no_bind_fingerprints_val = compute_fingerprints(scanner_no_bind, valid_indices_no_bind)\n",
    "bind_fingerprints_train = compute_fingerprints(scanner_bind, train_indices_bind)\n",
    "bind_fingerprints_val = compute_fingerprints(scanner_bind, valid_indices_bind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.concatenate([no_bind_fingerprints_train, bind_fingerprints_train])\n",
    "val_data = np.concatenate([no_bind_fingerprints_val, bind_fingerprints_val])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biosc1540-2024s-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
